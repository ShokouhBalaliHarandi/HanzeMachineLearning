{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Description\n",
    "\n",
    "This assignmebt involves informed decision-making and extraction from breast cancer data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data & Exploratory analysis\n",
    "\n",
    "For the first step breast cancer data (in a CSV file) are loaded and the first few rows to understand what the data looks like are shown. This helps to identify the columns, data types, and initial observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Import the data\n",
    "configPath = 'config.yaml'\n",
    "\n",
    "# Read the yaml data from the file\n",
    "with open(configPath, 'r') as file:\n",
    "    configData = yaml.safe_load(file)\n",
    "\n",
    "df = pd.read_csv(configData[\"breast_cancer_path\"])\n",
    "\n",
    "# Displaying the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Visualize the distribution of features\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.drop(['id'], axis=1).hist(bins=30, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check class balance\n",
    "class_counts = df['diagnosis'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as reults shows the diagnosis column is a categorical field involves 357 observed 'B' value and 212 observed 'M' value. And due to histograms' right tail and std info which shows spread of numerical features (from around 0.02 to near 350) the data are not normal. Furthermore, from the displaying data the id field  doesn't seem is valuable and meaningful field for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data\n",
    "\n",
    "in the following block to prepare the data for machine learning algorithms, the categorical target variable (diagnosis) is encoded to make it more suitable for modeling. Also the dataset into features (X) and the target variable (y) for further processing are splitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the diagnosis column (Malignant = 1, Benign = 0)\n",
    "label_encoder = LabelEncoder()\n",
    "df['diagnosis'] = label_encoder.fit_transform(df['diagnosis'])\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop(['id', 'diagnosis'], axis=1)\n",
    "y = df['diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above results show data are not normal, to improve the perfoemance of model features with high skewness (above 0.7) are identified and a power transformation is applied to those features. Finally, to ensure that features have similar scales, they scaled with StandardScaler to prevent of damage analysing because of big differences in the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Calculating skewness of the features\n",
    "skewness = X.skew()\n",
    "\n",
    "# Selecting features with skewness above a threshold (0.7)\n",
    "skewed_features = skewness[abs(skewness) > 0.7].index\n",
    "\n",
    "# Applying power transformation \n",
    "power_transformer = PowerTransformer()\n",
    "X_skewed = X[skewed_features].copy()\n",
    "X_skewed_transformed = power_transformer.fit_transform(X_skewed)\n",
    "\n",
    "# Replacing the original skewed features with the transformed features\n",
    "X[skewed_features] = X_skewed_transformed\n",
    "\n",
    "# Normalizing the features using standard scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling\n",
    "\n",
    "This step is for selecting machine learning algorithms for classification. The following code experiments with a Decision Tree Classifier and a Gaussian Naive Bayes Classifier over breast cancer data. By using cross-validation with cross_val_score, the performance of each classifier using accuracy scores is estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the classifiers with different parameters\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    GaussianNB()\n",
    "]\n",
    "\n",
    "# Train and evaluate the classifiers\n",
    "for classifier in classifiers:\n",
    "    scores = cross_val_score(classifier, X_scaled, y, cv=5)\n",
    "    print(f'{classifier.__class__.__name__}: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score represents the ratio of correctly predicted instances to the total number of instances for diagnosis target. The DecisionTreeClassifier results shows about 89.81% accuracy of the instances in the breast cancer dataset during cross-validation, meanwhile the the Gaussian Naive Bayes algorithm achieved an average accuracy score of around 0.9403. \n",
    "\n",
    "The higher accuracy of Gaussian Naive Bayes suggests that it might be better to the patterns present in the breast cancer dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating\n",
    "\n",
    "Accuracy alone might not provide a complete picture of model performance, especially in imbalanced datasets. To gain deeper insights, additional metrics (precision, recall, and F1-score using cross_val_predict) are used. These metrics give a better understanding of how well the models perform in terms of correctly classifying malignant 'M' and benign 'B' cases in the specific breast cancer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Perform cross-validation predictions\n",
    "for classifier in classifiers:\n",
    "    y_pred = cross_val_predict(classifier, X_scaled, y, cv=5)\n",
    "    print(f'{classifier.__class__.__name__} metrics:')\n",
    "    print(f'Accuracy: {accuracy_score(y, y_pred)}')\n",
    "    print(f'Precision: {precision_score(y, y_pred)}')\n",
    "    print(f'Recall: {recall_score(y, y_pred)}')\n",
    "    print(f'F1-Score: {f1_score(y, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the metrics between the two models:\n",
    "\n",
    "The Gaussian naive bayes model generally performs better across all metrics, with higher accuracy, (precision, recall, and F1-score) compared to the decision tree model.\n",
    "\n",
    "The decision tree model has a slightly higher precision (indicating fewer false positives) but lower recall (indicating fewer true positives) compared to the gaussian model. \n",
    "\n",
    "The F1-scores for both models are quite close, shows that the both models have similar overall performance in terms of the balance between precision and recall.\n",
    "\n",
    "In medical applications like breast cancer diagnosis, a higher recall might be more important than precision, as missing a true positive (a malignant 'M' case) can have serious consequences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "Finally, a Decision Tree Classifier (with a maximum depth of 3) is fitted. Visualizing the decision tree helps to understand how the model makes decisions based on different features over diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Create and fit a decision tree with max_depth=3\n",
    "decision_tree = DecisionTreeClassifier(max_depth=3)\n",
    "decision_tree.fit(X_scaled, y)\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(decision_tree, feature_names=X.columns, class_names=label_encoder.classes_, filled=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree visualization shows the structure of the decision tree model that has been trained on breast cancer dataset. Each node in the tree represents a decision based on a specific feature, and the branches leading to different nodes represent the possible outcomes of that decision.  \n",
    "\n",
    "Nodes near the root play a significant role in making decisions.\n",
    "\n",
    "in decision tree each node evaluates a specific feature and compares it against a threshold. If the feature value satisfies the condition, the algorithm follows the left branch. Otherwise, it follows the right branch. The process continues until reaching a leaf node, where the final prediction (Malignant 'M' or Benign 'B') is made.\n",
    "\n",
    "Furthermore, the visualization uses colors to represent different classes (malignant 'M' class and benign 'B' class), to make it easier to see how instances are classified at different points in the tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
