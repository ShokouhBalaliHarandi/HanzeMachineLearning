{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: What are common preprocessing steps? Explain for each step why and when you should execute this step and when not.\n",
    "---) Collecting Data: Collecting data from the external source in order to clean data and then provide it to use.\n",
    "\n",
    "\n",
    "---) Cleaning Data: Always there is a possibilty to be some impurity in data; therefore we have to choose some stategies to handle them. For instance:\n",
    "\n",
    "  *: replace them by some other values based on the project aim (if it deserves)\n",
    "\n",
    "  *: remove nan/na columns or rows\n",
    "\n",
    "Totally handeling missing values, remove noise, handling outliers, fixing data inconsistencies, mixed types, duplicates, mismatches, etc are done in this part.\n",
    "\n",
    "  -> If a dataset has highly relevant features may some of above list don't need.\n",
    "\n",
    "\n",
    "---) Handling the Categorical data: converting labelled data into numerical ones is more usefull for a machine to understand it. There is some approaches about this:\n",
    "\n",
    "  *: Label Encoding: for instanse converting true/false to 1/0\\\n",
    "\n",
    "  *: expanding multi labelled value columns: for example if dataset has a column by status name with 3 different labelled values we could expand it to 3 columns and by 0/1 shows the status of each row.\n",
    "\n",
    "  -> Some algorithms can handle categorical variables, but many ones require numerical inputs. If a dataset consists entirely of numerical features, it is possible to skip this step.\n",
    "\n",
    "\n",
    "---) Integrating Data: if we have different sources may it need to combine them.\n",
    "\n",
    "  -> If we have just a single source of data, this step may not be required.\n",
    "\n",
    "\n",
    "---) Transforming data/Feature Scaling: convert all values into same magnitude level to prevent of confusing model. For this part we have to normalize/standardize the data or smoothing them.\n",
    "\n",
    "  -> Some algorithms like tree-based models that are not affected by feature scaling, so it may not be necessary in those cases.\n",
    "\n",
    "\n",
    "---) Reductind Data: minimizing complexity of data with PCA to reduce dimension or clustering, sampling, feature selection, etc up to the data and the project aim/process.\n",
    "\n",
    "  -> If a dataset has only a few features, we may could skip this step.\n",
    "  \n",
    "\n",
    "##### Due to link: #https://scikit-learn.org/stable/modules/preprocessing.html the total steps divided in the following list:\n",
    "\n",
    " Standardization, or mean removal and variance scaling, Non-linear transformation, Normalization, Encoding categorical features, Discretization, Imputation of missing values, Generating polynomial features, Custom transformers\n",
    "\n",
    "\n",
    "In conclusion, execution of these preprocessing steps can vary depending on the specific requirements of an analysis or modeling task, as well as the nature and characteristics of your dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: What visualization methods are used in the cluster methods tutorial? Explain why the selected method is the most appropriate method for the visualization. Bonus points: do this as well for the scanpy tutorial.\n",
    "\n",
    "In the cluster methods tutorial, the following visualization methods are used:\n",
    "\n",
    "---) Histogram Plot: The histogram plot shows how red and white wines are rated. It helps us see how the quality scores are spread out for each type of wine. This method is good for comparing the quality of red and white wines because it uses separate histograms and colors to make the comparison clear and easy.\n",
    "\n",
    "\n",
    "---) Pairplot: The pairplot helps us see how the chemical properties of the wines relate to each other. It uses scatter plots and histograms to show the distributions and relationships between these properties. By looking at the it, we can identify patterns or groups in the data. This method is good because it gives us a complete picture of the data and makes it easier to assess scaling and normalization efforts.\n",
    "\n",
    "\n",
    "---) Dendrogram: The dendrogram helps us see the results of hierarchical clustering. It shows how clusters are linked together at different levels of similarity or distance. By looking at the dendrogram, we can identify separate clusters and understand the hierarchical structure of the data. This method is good for visualizing clustering because it helps us understand how clusters are formed and the relationships between them.\n",
    "\n",
    "In the scanpy tutorial, the following visualization methods are used:\n",
    "\n",
    "\n",
    "---) Scatter Plot: In the scanpy tutorial, scatter plots are used a lot to show how genes are expressed in single-cell RNA sequencing data. These plots give us a 2D view of the data, where each point represents a single cell. The position of the point is determined by the expression levels of certain genes. Scatter plots help us find cell clusters, see different cell types, and study gene expression patterns. They are the best way to visualize single-cell data because they let us explore relationships between multiple genes and cells all at once.\n",
    "\n",
    "\n",
    "---) Heatmap: Heatmaps are used to show how genes are expressed in different cell clusters or conditions. They use colors to represent expression levels, with different colors indicating high or low expression. Heatmaps help us find genes that are expressed differently, see patterns in gene expression, and compare expression across clusters or conditions. This method is good for visualizing gene expression data because it lets us look at multiple genes and conditions at the same time in a compact and informative way.\n",
    "\n",
    "\n",
    "---) Violin Plot: Violin plots are used to show how gene expression is distributed within each cell cluster. They combine a box plot and a kernel density plot, giving us information about both the summary statistics (like the median and quartiles) and the density of expression values. Violin plots help us see differences in gene expression between clusters and understand expression variability within clusters. This method is good for visualizing gene expression distributions because it gives us a more detailed representation than just traditional box plots.\n",
    "\n",
    "Overall, I think visualizing methods depend on the nature of the project process or analysis.\n",
    "\n",
    "#https://towardsdatascience.com/best-practices-for-visualizing-your-cluster-results-20a3baac7426, \n",
    "\n",
    "#https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_display_object_visualization.html\n",
    "\n",
    "\n",
    "For example for Scanpy: (#https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)\n",
    "\n",
    "The Elbow method may not be directly proper for determining the optimal number of clusters. Instead, Scanpy provides some other methods to evaluate the number of clusters, such as silhouette, iterative refinement approaches.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: What performance/evaluation metrics are in the cluster methods tutorial? Explain why the used methods are the most appropriate method for the evaluation.\n",
    "Due to cluster methods tutorial, the following metrics are used to evaluate the performance of the clustering and classification tasks:\n",
    "\n",
    "---) Inertia: Inertia is a metric used in K-means clustering to assess clustering quality. It calculates the sum of squared distances between each sample and its nearest centroid. A lower inertia value indicates better clustering performance. Inertia is suitable for K-means clustering as it measures how well samples are assigned to clusters and the compactness of the clusters.\n",
    "\n",
    "\n",
    "---) Cluster Counts: The tutorial counts the number of red and white wines in each cluster for both K-means and agglomerative clustering. This metric helps understand the composition and distribution of clusters based on wine colors. It provides insights into how well the clustering algorithms separate wines by color and identifies any potential imbalances or biases in the results.\n",
    "\n",
    "\n",
    "---) ROC-AUC Score: The ROC-AUC score is used to evaluate the performance of Random Forest classifiers trained on the clustered data. It measures the classifier's ability to distinguish between positive and negative classes by calculating the area under the ROC curve. This metric is suitable for classification tasks, such as predicting whether a wine quality is above 7 or not. It considers both the true positive rate and false positive rate, providing a comprehensive assessment of the classifier's performance.\n",
    "\n",
    "These evaluation metrics are appropriate for the cluster methods tutorial as they assess different aspects of clustering and classification tasks. Inertia measures clustering quality and compactness. Cluster counts provide information about cluster composition based on wine color. ROC-AUC score evaluates the classification performance of Random Forest classifiers using clustered features. Together, these metrics offer a comprehensive evaluation of clustering and classification results, enabling an assessment of the effectiveness and accuracy of the methods employed.\n",
    "\n",
    "Due to #https://analyticsindiamag.com/a-tutorial-on-various-clustering-evaluation-metrics/\n",
    "\n",
    "---) Silhouette Coefficient: It measures the quality of clustering by evaluating both the cohesion and separation. It ranges from -1 to 1, with higher values indicating better-defined clusters.\n",
    "\n",
    " The Silhouette Coefficient is useful for assessing the overall quality and separation of clusters, taking into account both cohesion and separation.\n",
    "\n",
    "\n",
    "---) Calinski Harabaz Index: It quantifies the separation between clusters. It measures both the cohesion within clusters and the separation between clusters to assess the quality of clustering results.\n",
    "\n",
    " One advantage of the Calinski-Harabasz Index is that it doesn't require a ground truth or reference clustering for evaluation. It solely relies on the data and the clustering results. However, it should be noted that this index tends to favor clustering algorithms that produce convex-shaped clusters.\n",
    " When comparing multiple clustering algorithms or evaluating different parameter settings, the Calinski-Harabasz Index can be used as a criterion to select the optimal clustering solution that maximizes the separation between clusters while minimizing the dispersion within clusters.\n",
    "\n",
    "\n",
    "---) Davies Bouldin index: It quantifies the compactness and separation of clusters. It measures the average similarity between each cluster and its most similar neighboring cluster while considering the internal scatter of the clusters.\n",
    "\n",
    " The DBI can be used as a measure to compare and select the best clustering solution among multiple algorithms or different parameter settings. It provides an indication of the quality of clustering by considering both compactness and separation aspects."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bouns: You practice the steps yourself with the breast_cancer dataset (clustering_data.csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Inspector (Import the data and examine the features)\n",
    "In the first step we have to load data and investigate them.\n",
    "\n",
    "To do this after loading data, we could print some rows as sample and then check the number of columns and rows (with shape command) and the datatype of them.\n",
    "\n",
    "Then checking columns if they are necessary for our problem or the type of columns are proper for our analysis. (These checking will help us in a prepration process of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Import the data\n",
    "configPath = 'config.yaml'\n",
    "\n",
    "# Read the yaml data from the file\n",
    "with open(configPath, 'r') as file:\n",
    "    configData = yaml.safe_load(file)\n",
    "\n",
    "data = pd.read_csv(configData[\"clustering_data_path\"])\n",
    "\n",
    "#get data base info\n",
    "print(f'shape of data: {data.shape}')\n",
    "\n",
    "# Display the first few rows of the data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types of each column\n",
    "print(data.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing Data\n",
    "\n",
    "After checking data, these tips are realized to prepare data before doing any analysis on them:\n",
    "\n",
    "-) regarding to the type of analysis the id column which seems be aa an unique identifier for each data row is not necessary and to reduce the column we could remove it.\n",
    "\n",
    "-) diagnosis column is type of categorical data; therefor, for better analysis we have to convert it to the numerical value. for this approach:\n",
    "    firstly: we have to find all unique values of diagnoses and them assign number to each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"id\" column as it is not relevant for clustering\n",
    "data = data.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work on diagnosis column because in the part of getdataTypes it is objects and k-mean just work by float values\n",
    "\n",
    "# Check the unique values in the \"diagnosis\" column\n",
    "print(data['diagnosis'].unique())\n",
    "\n",
    "# Convert the \"diagnosis\" column to binary values (M: 1, B: 0)\n",
    "data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Check the number of entries for each diagnosis\n",
    "print(data['diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat checking data:\n",
    "\n",
    "After doing some preprocessing functions and fix columns for our analysis in terms of columns datatype, we must checking them agian to understand the status of whole data in terms of their distrubution status\n",
    "\n",
    "for doing this we could use multiple methods such as using .discribe() method or doing some plots such as histogram.\n",
    "Furthermore, we could check the correlation of data to understand the variables (columns) relationship to be able select most relevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the statistics of the numerical features\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result of discribe method we could get a quick overview of distrbution of data of status of missing values. for instance, The mean shows central tendency, however with std (standard division) we could understand about spread of the data. and min, max, and quarters value can help to guess the propability of outliners.\n",
    "\n",
    "But we have to be aware about diagnosis column because it is categorical columns with value of 0 and 1 (after preprocessing) so the describe method can't useful for the statistics like mean or std . Instead, it gives other data like the number of non-null values (count = 569).\n",
    "\n",
    "It is noticable as count value calculates non-null values and in previouse steps we get the number of rows as 569 so the colummns (variables) which has count equal by 569 that means that they have no nun-value or missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we could get some of those information by ploting histogram or boxplot or other type of diagrams. Just for example I plot histogram on diagnosis and radius_mean to show how could we plot and interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the \"diagnosis\" column for the histogram\n",
    "diagnosis = data['diagnosis']\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(diagnosis, bins=20, edgecolor='black')\n",
    "plt.xlabel('diagnosis')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of diagnosis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram shows that diagnosis just have to values B mapped to 0 is around 350 and M mapped to 1 around 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the \"radius_mean\" column for the histogram\n",
    "radius_mean = data['radius_mean']\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(radius_mean, bins=20, edgecolor='black')\n",
    "plt.xlabel('Radius Mean')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Radius Mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The radius_mean histogram shows that data are not normalize because they are not gathered on the central tendency of data and also there is some outerlines mostly in the right tail of histogram because there is some isolate bulk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we could plot the histogram of the radius_mean (or other factors) for diagnosis values (as a categorical variable) to compare the distributions of radius_mean for diagnosis cases. This type of inspectoring could useful in some specific problems (to find distribution of some specific factor around another. However, for this assignment I just wanted to show how these type of plots work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram of radius for diagnosis values (M, B)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the data based on diagnosis values\n",
    "diagnosis_M = data[data['diagnosis'] == 1]\n",
    "diagnosis_B = data[data['diagnosis'] == 0]\n",
    "\n",
    "# Plot histogram for diagnosis M\n",
    "plt.hist(diagnosis_M['radius_mean'], bins=20, edgecolor='black', alpha=0.5, label='M(=1)')\n",
    "# Plot histogram for diagnosis B\n",
    "plt.hist(diagnosis_B['radius_mean'], bins=20, edgecolor='black', alpha=0.5, label='B(=0)')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Radius Mean')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Radius Mean by Diagnosis')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below, the histograms of all numerical values are plotted. This step can be an alternative method to have quick review of distrbution data. \n",
    "\n",
    "The result of .discrube() or plots can help us to make dicision for normalize/standardize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of each feature\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i, column in enumerate(data.columns[2:], 1):\n",
    "    plt.subplot(5, 6, i)\n",
    "    sns.histplot(data[column], kde=True)\n",
    "    plt.xlabel(column)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again like discribe method, plots show us data have not normal normalization and most have right tail and have some outliners. Beside, there is a column unnamed has non-value column.\n",
    "\n",
    "It is noticable that outliners are recognizable at isolate bulks or end tails or in the columns with weird higher frequency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling & Visualizing the pairwise distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we are following these steps:\n",
    "\n",
    "- ) checking the correlation between columns (variables) to find the relation between them to pick most important of them.\n",
    "\n",
    "- ) as we examined the distrb. of data in above and understood they need scale, because mostly they have right tail. But in the following I used a skew function to have an statistical method beside the visualization to be find the long tail variables to transform those data because the real value of data may harm of our process regarding their wide variation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### correlation part:\n",
    "\n",
    "calculate correlations between all varialbe in pair with corr function and checking variables with highly correlated based on their correlation coefficients. Then, checking if any features can be omitted based on the specific threshold (0.7) for high correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Select the relevant variables (excluding diagnosis columns)\n",
    "relevant_variables = data.drop(['diagnosis'], axis=1)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = relevant_variables.corr()\n",
    "\n",
    "print(correlation_matrix.abs().idxmax())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "due to correlation_matrix.abs().idxmax() result (all columns has 1 correlative amount with themselves), I used another calculation method for picking correlated features with specific threshold.\n",
    "\n",
    "High correlated shows variables that have a strong relationship with each other. Therefore, they carry almost the same information, so we could eleminate them to prevent of multicollinearity problem. However in this assignment I don't eleminate variables because I wasn't sure about covariate of data and I guess maybe we need to have covariates data to be sure about equality of data measurment or even scaling them. (As experience in Omics projects). I just use this step to show the process but I don't use the result of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation threshold\n",
    "threshold_Cor = 0.7\n",
    "\n",
    "# Find highly correlated features\n",
    "highly_Correlated = set()\n",
    "\n",
    "# Iterate over the correlation matrix\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold_Cor:\n",
    "            # Get the names of the highly correlated features\n",
    "            feature_i = correlation_matrix.columns[i]\n",
    "            feature_j = correlation_matrix.columns[j]\n",
    "            \n",
    "            # Add the feature names to the set\n",
    "            highly_Correlated.add(feature_i)\n",
    "            highly_Correlated.add(feature_j)\n",
    "\n",
    "# Display the highly correlated features\n",
    "print(f\"Highly {len(highly_Correlated)} Correlated Features\")\n",
    "print(highly_Correlated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skewness part\n",
    "\n",
    "Regarding illustration under Scaling & Visualizing the pairwise distributions, we have to find high skew variable to transform their data. This function used a threshold for it's aim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the skewness \n",
    "skewness = relevant_variables.skew().sort_values(ascending=False)\n",
    "\n",
    "#print(\"Skewness of variables\")\n",
    "#print(skewness)\n",
    "\n",
    "threshold_Skew = 0.7\n",
    "skew_Cols = skewness.loc[skewness > threshold_Skew]\n",
    "print(\"skew columns\")\n",
    "print(skew_Cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calculating the skew values, we can see which variables are highly skewed and to improve the performance of the rest analysis we transfer data with log to reduce the skewness and make distributions symmetrical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transform on skewed columns\n",
    "for col in skew_Cols.index.tolist():\n",
    "    data[col] = np.log1p(data[col])\n",
    "    \n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before clustering the data, we should scale it. Because most clustering algorithms use distances to find similarities between data points. When features have big differences in variance, the larger ones can cause damage result in a biased manner.\n",
    "\n",
    "##### I want to notice that regarding illustration of corr() part and the reseon that I don't eleminate most correlated (high correlation) variables, the variable of this part are too much so the each execution is get longer time and besides the plots of variables are more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate the features from the target variable\n",
    "X = data.drop('diagnosis', axis=1)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)\n",
    "\n",
    "# Create a new DataFrame with the scaled features\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Add the target variable back to the DataFrame\n",
    "X_scaled['diagnosis'] = data['diagnosis']\n",
    "\n",
    "# Check the scaled data\n",
    "print(X_scaled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Set the color palette\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "# Plot the pairwise distributions of the features\n",
    "sns.pairplot(X_scaled, hue='diagnosis')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As this figuers shows, variables has correlation 1 with each other and the linear correlation (or almost linear) are variables with high values and we calculated them before but don't omit them from the rest of our assignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means Clustering\n",
    "\n",
    "To do this I:\n",
    "\n",
    "-) Dropped the diagnosis because it was a categorical and such a target (depend) variable and also kmean in meaningful for numerical variables.\n",
    "\n",
    "-) imputed missing values with KNNImputer to prevent bias analysis and help ML algorithms to handle missing values. I don't drop missing values before because I think using KNNImputer which works with k nearest neighbors is more effective and proper. (I used k=2)\n",
    "\n",
    "-) Initialized the KMeans algorithm with 2 clusters for the data and then fit it based on the imputed variables.\n",
    "\n",
    "-) added a new column (cluster) to the scaled DataFrame to determine the cluster labels calculated with KMeans.\n",
    "\n",
    "-) finally to get the quick sense of calculated data, I printed the population status of diagnoses in each clauster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Separate the features from the target variable\n",
    "X_features = X_scaled.drop('diagnosis', axis=1)\n",
    "\n",
    "# handle null values\n",
    "k=2 \n",
    "imputer = KNNImputer(n_neighbors=k)\n",
    "imputed_data = imputer.fit_transform(X_features)\n",
    "\n",
    "# Fit the K-means clustering model\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(imputed_data)\n",
    "\n",
    "# Add the cluster labels to the DataFrame\n",
    "X_scaled['cluster'] = kmeans.labels_\n",
    "\n",
    "# Examine the clusters by counting the number of Malignant (M) and Benign (B) samples in each cluster\n",
    "print(X_scaled.groupby(['cluster', 'diagnosis']).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we reach to the 2 clusters labled 0,1 and in first cluster we have 344 diagnoses 0 (=B) and 33 diagnosis 1 (=M), which means totally 377 diagnosis in this cluster. In the other cluster there is 179 diagnosis involves 13 diagnosis 0 and 188 diagnonsis 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### @https://github.com/fenna/BFVM23DATASCNC5/blob/main/Tutorials/tutorial_Clustering_Methods.ipynb\n",
    "\n",
    "Our previouse code just work on 2 clusters. But if we want to find the better K for clustering in the Kmean we could fit kmean with cluster values in wide rang, for example 1:20. (elbow method) and plotting cluster number vs inertia (sum of squared distances of samples to their closest cluster center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the range of cluster values\n",
    "cluster_values = range(1, 21)\n",
    "\n",
    "# Initialize lists to store the number of clusters and inertia values\n",
    "num_clusters = []\n",
    "inertia_values = []\n",
    "\n",
    "# Iterate over the cluster values\n",
    "for k in cluster_values:\n",
    "    # Create a KMeans model with k clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    kmeans.fit(imputed_data)\n",
    "    \n",
    "    # Append the number of clusters and inertia value to the respective lists\n",
    "    num_clusters.append(k)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot cluster number vs. inertia\n",
    "plt.plot(num_clusters, inertia_values, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('K-Means: Cluster Number vs. Inertia')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regarding to the plot it seems we have an elbow point arount point 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering using Agglomerative Clustering\n",
    "\n",
    "Agglomerative clustering algorithm is used to perform hierarchical clustering on the imputed_data. The steps of this method is like Kmean:\n",
    "\n",
    "-) initialing algorithm with 2 clusters.\n",
    "\n",
    "-) fitting the model.\n",
    "\n",
    "-) adding the agglom_cluster column to save cluster labels.\n",
    "\n",
    "-) Printing the population status of diagnoses in each clauster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Fit the Agglomerative Clustering model\n",
    "agglomerative = AgglomerativeClustering(n_clusters=2)\n",
    "agglomerative.fit(imputed_data)\n",
    "\n",
    "# Add the cluster labels to the DataFrame\n",
    "X_scaled['agglom_cluster'] = agglomerative.labels_\n",
    "\n",
    "# Examine the clusters by counting the number of Malignant (M) and Benign (B) samples in each cluster\n",
    "print(X_scaled.groupby(['agglom_cluster', 'diagnosis']).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this method, we reach to the 2 clusters labled 0,1 and in first cluster we have 52 diagnoses 0 (=B) and 195 diagnosis 1 (=M), which means totally 247 diagnosis in this cluster. In the other cluster there is 322 diagnosis involves 305 diagnosis 0 and 17 diagnonsis 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to compare and evaluate the quality of clustering results between kmean and Agglomerative I used Silhouette which help to recognize how well-separated the clusters are and similar the data points are within their own clusters compared to other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Evaluate the K-means clustering using silhouette score\n",
    "silhouette_score_kmeans = silhouette_score(imputed_data, kmeans.labels_)\n",
    "print(\"Silhouette Score (K-means):\", silhouette_score_kmeans)\n",
    "\n",
    "# Evaluate the Agglomerative Clustering using silhouette score\n",
    "silhouette_score_agglomerative = silhouette_score(imputed_data, agglomerative.labels_)\n",
    "print(\"Silhouette Score (Agglomerative Clustering):\", silhouette_score_agglomerative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As results show the Agglomerative algorithm do better clustring with proper sepration of data, because its score is higher (0.28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I do some scatter plot to check the new status between variables after calculations. As as diagnosis is a categorical variable with 0 and 1 values, therefore, the points are concentrated in two lines 0 and 1. Because of this in the last code block I didn't put the diagnosis columns in the plotting. These plots just for sample to show how we could check new status of data after analysis and get more info about them. However depends on the data and problem we could do this part in many different meaningful ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_scaled['texture_mean'], X_scaled['area_mean'], c=X_scaled['cluster'])\n",
    "plt.xlabel('texture_mean')\n",
    "plt.ylabel('area_mean')\n",
    "plt.title('K-means Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of diagnosis and radius_mean which tested in histogram\n",
    "plt.scatter(X_scaled['diagnosis'], X_scaled['radius_mean'], c=X_scaled['cluster'])\n",
    "plt.xlabel('diagnosis')\n",
    "plt.ylabel('radius_mean')\n",
    "plt.title('K-means Clustering')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of column names excluding 'diagnosis'\n",
    "columns = [col for col in X_scaled.columns if col != 'diagnosis']\n",
    "\n",
    "# Iterate over each column\n",
    "for col in columns:\n",
    "    plt.scatter(X_scaled['radius_mean'], X_scaled[col], c=X_scaled['cluster'])\n",
    "    plt.xlabel('radius_mean')\n",
    "    plt.ylabel(col)\n",
    "    plt.title(f'K-means Clustering: {col} vs. radius_mean')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
