{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Description:\n",
    "\n",
    "The aim of the assignment is to perform clustering on the breast cancer dataset to explore patterns and groupings in the data to explore if there are distinct subtypes of breast cancer based on their clinical features?\n",
    "\n",
    "To do this aim, I did these steps in the following:\n",
    "\n",
    "1- Data Loading and Preprocessing\n",
    "\n",
    "2- K-Means Clustering\n",
    "\n",
    "3- Visualization with Dimension Reduction\n",
    "\n",
    "4- Hyperparameter Tuning and Evaluation\n",
    "\n",
    "5- Comparison of Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading and preparing data\n",
    "\n",
    "To assess a structure of the breast cancer dataset, the following steps were done:\n",
    "\n",
    "1- load the data\n",
    "\n",
    "2- inspecting its shape. The shape attribute of the loaded dataset helps to get the information about the number of observations (samples) and features (attributes) in the dataset. By getting shape we understand the balance between observations and features. If we have a balanced dataset, therefore we could be sure that we have proper data for clustering and gain meaningful patterns.\n",
    "\n",
    "3- checking data (head command) and compare it with the data type of columns are also usefull to reach if data are in the correct format.\n",
    "\n",
    "4- check the normalization of data. By checking this we can understand the preprocessing approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "#Import the data\n",
    "\n",
    "configPath = 'config.yaml'\n",
    "\n",
    "# Read the yaml data from the file\n",
    "with open(configPath, 'r') as file:\n",
    "    configData = yaml.safe_load(file)\n",
    "\n",
    "df = pd.read_csv(configData[\"breast_cancer_path\"])\n",
    "\n",
    "print(df.shape) # get the size of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head()) # Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes) # get the type of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of each feature\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i, column in enumerate(df.columns[2:], 1): #diagnose field is not plotted because it is categorical variable\n",
    "    plt.subplot(5, 6, i)\n",
    "    sns.histplot(df[column], kde=True)\n",
    "    plt.xlabel(column)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing data\n",
    "\n",
    "from the above 'diagnosis' is a categorical field and other with float type are features; So I assume 'diagnosis' as a dependent variable. \n",
    "\n",
    "Besides, as in the above showed, data has a right tail and have not normal normalization. So I used scale data to prevent of harming the rest of process regarding their wide variation. \n",
    "\n",
    "Regarding the above results, Firstly I seprate the diagnonsis as a dependent variable (Y) and then scale other features. and finally concat the Y to the final scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = df.drop(['diagnosis'], axis=1)\n",
    "y = df['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create a new DataFrame with the scaled features\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Concatenate the scaled features with the target variable\n",
    "df_preprocessed = pd.concat([df_scaled, y], axis=1)\n",
    "\n",
    "# Display the first few rows of the preprocessed dataset\n",
    "print(df_preprocessed.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define a pipeline with preprocessing and clustering\n",
    "\n",
    "Regarding to the assignment question written in the begining (if there are distinct subtypes of breast cancer based on their clinical features?) and since we don't have any definition for probability subtypes so supervise method was chosen and the common method to recognize clustering is k-mean.\n",
    "\n",
    "These clusters may help in representing groups of patients with distinct clinical characteristics, leading to a deeper understanding of disease and their treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to find the better K for clustering in the Kmean we could fit kmean with cluster values in wide rang (1:20). (elbow method) and plotting cluster number vs inertia (sum of squared distances of samples to their closest cluster center)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define the range of cluster values\n",
    "cluster_values = range(1, 21)\n",
    "\n",
    "# Initialize lists to store the number of clusters and inertia values\n",
    "num_clusters = []\n",
    "inertia_values = []\n",
    "\n",
    "# Iterate over the cluster values\n",
    "for k in cluster_values:\n",
    "    # Create a KMeans model with k clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    kmeans.fit(X_scaled)\n",
    "    \n",
    "    # Append the number of clusters and inertia value to the respective lists\n",
    "    num_clusters.append(k)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot cluster number vs. inertia\n",
    "plt.plot(num_clusters, inertia_values, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('K-Means: Cluster Number vs. Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the result it suggested that 3 clustering could be proper for k-mean method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = df_preprocessed.drop(['diagnosis'], axis=1)\n",
    "y = df_preprocessed['diagnosis']\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Preprocessing step: feature scaling\n",
    "    ('clustering', KMeans(n_clusters=3))  # Clustering algorithm: KMeans\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline.fit(X)\n",
    "\n",
    "# Perform clustering and obtain the labels\n",
    "cluster_labels = pipeline.predict(X)\n",
    "\n",
    "# Display the cluster labels\n",
    "print(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aboved result shows the 3 different cluster based on bio characters on the refrence dataset named 0, 1, 2. As understanding the diagnosis status in these cluster I used the above grouping to undesrtand the distribution of the diagonsis in them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataframe to save clustering result and diagnosis\n",
    "df_kmean_inv = pd.DataFrame()\n",
    "df_kmean_inv['cluster'] = cluster_labels\n",
    "df_kmean_inv['diagnosis'] = y\n",
    "\n",
    "# Examine the clusters by counting the number of Malignant (M) and Benign (B) samples in each cluster\n",
    "print(df_kmean_inv.groupby(['cluster', 'diagnosis']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above result shows, in the third clustering we just have 'M' category of diagnosis. and most observations are in 'B' type in the first clustering.\n",
    "\n",
    "To understand the validation of the result, I used Silhouette method which measures the similarity of each data point to its assigned cluster compared to neighboring clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate the silhouette score\n",
    "silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "\n",
    "# Display the silhouette score\n",
    "print(\"Silhouette Score:\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Silhouette Score of 0.3067 is moderately positive, shows that there is some degree of separation among the clusters, but there is also room for improvement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension reduction and visualization with pca and tsne\n",
    "\n",
    "To visualize the clustering result in the following PCA and Tsne with 2 components are used. 2 components were chosen because it is common choice in a 2D space to visualize the data points and their clustering relationship more easily. Also, usually, 2 first components capture the most variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform dimension reduction using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Perform dimension reduction using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize using PCA\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels)\n",
    "plt.title('PCA Visualization')\n",
    "\n",
    "# Visualize using t-SNE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_labels)\n",
    "plt.title('t-SNE Visualization')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both diagram show the observed datapoints in reduced space with first and second components (features). and each color shows the cluster of the datapoint. The result of these diagram are similar by the result of the silhouette Score. Because data are mostly in their cluster and there is just a few noise and overfitting between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to improve kmean result, one of the ways is checking the features and choose the most relevent components to prevent of effecting quality of clustering by features, we could cumulative pca results of the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA() #I don't mention the number of component to calculate all\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "print(f'all components are: {len(variance_ratio)} and the status of covring the variance are shown in the following: \\n {variance_ratio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the above there are 31 component and depends on the problem we can select the top n components for example if we choose the 0.9 coverage thershold, then it seems 7 first component are suffietient.\n",
    "\n",
    "Since we eleminate 'diagnosis' feature in the clustering and regarding the shape() command we have 32 columns so the 31 component is correct in the result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV for hyperparameter\n",
    "\n",
    "To perform a hyperparameter tuning process to find the optimal hyperparameters for the K-Means clustering algorithm. (the proper clustering number), the GridSearchCv was used. Because if we want to do it manually and fit the model by that it will take a long time and also may not completely accurate. (The aim is to find the value of k that leads to the most meaningful and biologically interpretable clustering solution.)\n",
    "\n",
    "To initial the method these parameters were used: \n",
    "'clustering__n_clusters': [3, 4, 5]: The number of clusters (k) we want the K-Means algorithm to create. (Regarding the elbow method.'s result) \n",
    "\n",
    "'clustering__max_iter': [100, 200, 300]: Maximum number of iterations the K-Means algorithm to converge to the final clustering solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# pipeline with preprocessing and clustering\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Preprocessing step: feature scaling\n",
    "    ('clustering', KMeans())  # Clustering algorithm: KMeans\n",
    "])\n",
    "\n",
    "# parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'clustering__n_clusters': [3, 4, 5],  # Number of clusters\n",
    "    'clustering__init': ['k-means++', 'random'],  # Initialization method\n",
    "    'clustering__max_iter': [100, 200, 300]  # Maximum number of iterations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5 = X #for sepraring the result of new clustering, I set the X in the X5\n",
    "\n",
    "# Perform hyperparameter tuning using GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X5)\n",
    "\n",
    "# Get the best hyperparameters and the best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "# Fit the best model to the data\n",
    "best_model.fit(X5)\n",
    "\n",
    "# Perform clustering and obtain the labels using the best model\n",
    "cluster_labels5 = best_model.predict(X5)\n",
    "\n",
    "# Display the best hyperparameters and the cluster labels\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Cluster Labels:\", cluster_labels5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the above results it is concluded that: \n",
    "\n",
    "The optimal number of maximum iterations for the K-Means algorithm is 100 and The optimal number of clusters chosen by the GridSearchCV is 5. This means that the K-Means algorithm will create five distinct clusters to group the breast cancer data points. Besides, during the clustering, the algorithm iterates a maximum of 100 times to update the cluster assignments and centroids to ensure that the algorithm converges to a stable solution.\n",
    "\n",
    "Finaly, the cluster labels obtained using the optimal hyperparameters have been provided as output. Each data point has been assigned to one of the five clusters based on the K-Means algorithm's clustering results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate performance\n",
    "\n",
    "The next step is to validate the clustering outcome biologically, as mentioned previously. To interpret the clustering results in the context of known biological subtypes and clinical features is crucial. Additionally, evaluating the clustering performance using evaluation metrics like the Silhouette Score or Calinski-Harabasz Index will further assess the clustering quality and its biological relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate the silhouette score\n",
    "silhouette_avg5 = silhouette_score(X5, cluster_labels5)\n",
    "\n",
    "# Display the silhouette score\n",
    "print(\"Silhouette Score:\", silhouette_avg5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calinski-Harabasz Index\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "# Calculate the Calinski-Harabasz index\n",
    "ch_score5 = calinski_harabasz_score(X5, cluster_labels5)\n",
    "\n",
    "# Display the Calinski-Harabasz index\n",
    "print(\"Calinski-Harabasz Index:\", ch_score5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Score: 0.165: This Score shows that the clusters have some degree of separation, but they are not very well-defined. The score is relatively low, indicating that there might be overlapping data points or ambiguous cluster boundaries.\n",
    "\n",
    "Calinski-Harabasz Index: 131.505: This score shows that there is some degree of separation between the clusters. However, it is important to note that this metric can be influenced by the number of clusters and the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method names for the plot\n",
    "methods = ['K-Means-3Cluster', 'K-Means-5Cluster']\n",
    "silhouette_scores = [silhouette_avg, silhouette_avg5]\n",
    "ch_scores = [ch_score5, ch_score5]\n",
    "\n",
    "# a bar plot for silhouette scores\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(methods, silhouette_scores)\n",
    "plt.title('Silhouette Score Comparison')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "# a bar plot for Calinski-Harabasz index scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(methods, ch_scores)\n",
    "plt.title('Calinski-Harabasz Index Comparison')\n",
    "plt.ylabel('Calinski-Harabasz Index')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we fount 5 cluster from the gridsearchcv but according to the plot we found kmean by 3 cluster has the better result in the clustering. This might because:\n",
    "\n",
    "GridSearchCV tries different hyperparameter combinations and evaluates the clustering performance using cross-validation. It takes into account the internal structure of the data and the cluster quality measures. It's possible that GridSearchCV found that 5 clusters have better performance according to the chosen evaluation metric (e.g., silhouette score) compared to the inertia-based elbow method. (3-clustering was based on elbow method)\n",
    "\n",
    "But as previously mentioned in both clustering, both clustering has some room to improve. It might be beneficial to explore alternative clustering algorithms, adjust the number of clusters, or consider feature engineering or selection techniques to enhance the clustering performance and uncover more meaningful patterns in the breast cancer data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
