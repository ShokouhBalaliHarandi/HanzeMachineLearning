{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Description\n",
    "\n",
    "This assignment's aim is evaluating the performance of a classification model using various metrics to understand the different evaluation techniques available and select the appropriate metrics based on the context of the problem.\n",
    "\n",
    "The dataset used for this assignment is the Breast Cancer dataset from the file breast-cancer.csv. The dataset contains information about various factors of breast cancer cells and their diagnosis (M = malignant, B = benign)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import yaml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading data, exploratory analysis & preparing data\n",
    "\n",
    "The first step is understanding data. It involves examining the data's structure, summary statistics, and distribution of features. Based on the result, some other prepration of data such a finding and managing missing values, outliers, or data preprocessing requirement.\n",
    "\n",
    "(Exploratory analysis for this dataset was done before in the first and second week assignment so in the following some parts or illustrations didn't repeat.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data\n",
    "configPath = 'config.yaml'\n",
    "\n",
    "# Read the yaml data from the file\n",
    "with open(configPath, 'r') as file:\n",
    "    configData = yaml.safe_load(file)\n",
    "\n",
    "data = pd.read_csv(configData[\"breast_cancer_path\"])\n",
    "\n",
    "print(data.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data info\n",
    "print(f'shape of data {data.shape}')\n",
    "print(data.describe())\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data for null values\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram of diagnosis (diagnoses is a dependet(=target) variable)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "data[\"diagnosis\"].value_counts().plot(kind=\"bar\")\n",
    "plt.xlabel(\"Diagnosis\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Diagnosis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot histograms of each feature\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i, column in enumerate(data.columns[2:], 1):\n",
    "    plt.subplot(5, 6, i)\n",
    "    sns.histplot(data[column], kde=True)\n",
    "    plt.xlabel(column)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to all above information how ever it doesn't seem that data has a null-values but regarding to std values that show spread of the data and right tail of histograms it seems that the data are not completely normalized.\n",
    "Furthermore, due to histogram of \"diagnoses\" the values of this column is \"M\" and \"B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing data\n",
    "\n",
    "Before modeling, the data must be preprocessed to ensure it is in a suitable format. This includes handling missing values, scaling features and splitting data into training and testing for models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the features (X) and the target variable (y)\n",
    "X = data.drop(\"diagnosis\", axis=1)\n",
    "y = data[\"diagnosis\"]\n",
    "\n",
    "# handling missing values (replace them with mean)\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "#Scaling the features (standardization)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling based on Logistic Regression and SVM models\n",
    "\n",
    "In this step, 2 classification algorithms, Logistic Regression and Support Vector Machines (SVM), are used for modeling. Different hyperparameter values are explored for each algorithm. Cross-validation technique (k-fold cross-validation) is used to assess model performance and select the best hyperparameter values.\n",
    "\n",
    "There are some features for these models that have to be set. However, selecting these feature depends on problem and doamin knowladges but in the following some feature are tried to calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Defining the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Defining a range of k values to try\n",
    "k_values = [3, 5, 7, 10] #these values are chosen because these types of folds are common.\n",
    "\n",
    "# Performing cross-validation and track average scores\n",
    "average_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    scores = cross_val_score(model, X, y, cv=k)\n",
    "    average_scores.append(scores.mean())\n",
    "\n",
    "best_k = k_values[average_scores.index(max(average_scores))]\n",
    "print(\"Best k value:\", best_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As commonly K for k-fold validation is 3, 5, 7, and 10 in the above codes all are tested to find the best k. Regarding the result the 10 is the best value for it.\n",
    "\n",
    "the hyperparameters for gamma and C for regularization svm and log-regession are assumed by [0.001, 0.01, 0.1, 1, 10, 100] to cover both low and high regularization strengths. \n",
    "\n",
    "The regularization parameter C in both Logistic Regression and SVM models controls the trade-off between fitting the training data well and keeping the model simple to prevent overfitting. In the previouse version just 0.1 and 1 and 10 were used, but now more values are used to get accuracy upper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the models with different hyperparameter values\n",
    "logreg_model = LogisticRegression()\n",
    "svm_model = SVC()\n",
    "\n",
    "C = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Defining the hyperparameter values to try for each model\n",
    "logreg_params = {'C': C}\n",
    "svm_params = {'C': C, 'gamma': C}\n",
    "\n",
    "k = 10  # Number of folds which calculated in the previouse section\n",
    "\n",
    "# Logistic Regression\n",
    "logreg_scores = []\n",
    "for param in logreg_params['C']:\n",
    "    logreg_model.C = param\n",
    "    scores = cross_val_score(logreg_model, X, y, cv=k)\n",
    "    logreg_scores.append(scores.mean())\n",
    "\n",
    "# SVM\n",
    "svm_scores = []\n",
    "for param_C in svm_params['C']:\n",
    "    for param_gamma in svm_params['gamma']:\n",
    "        svm_model.C = param_C\n",
    "        svm_model.gamma = param_gamma\n",
    "        scores = cross_val_score(svm_model, X, y, cv=k)\n",
    "        svm_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare and select the best model based on cross-validation scores\n",
    "best_logreg_param = logreg_params['C'][logreg_scores.index(max(logreg_scores))]\n",
    "best_svm_param_C, best_svm_param_gamma = svm_params['C'][svm_scores.index(max(svm_scores)) // len(svm_params['gamma'])], \\\n",
    "                                        svm_params['gamma'][svm_scores.index(max(svm_scores)) % len(svm_params['gamma'])]\n",
    "\n",
    "print(\"Logistic Regression cross-validation scores:\", logreg_scores)\n",
    "print(\"Best Logistic Regression hyperparameter (C) value:\", best_logreg_param)\n",
    "print(\"SVM cross-validation scores:\", svm_scores)\n",
    "print(\"Best SVM hyperparameter (C, gamma) values:\", best_svm_param_C, \",\", best_svm_param_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation scores for logistic regression are as follows: [0.899, 0.952, 0.973, 0.982, 0.975, 0.967]. These scores indicate the performance of the logistic regression model with different values of the hyperparameter 'C'. And The best performing of that has a cross-validation score of approximately 0.9824, and the corresponding hyperparameter 'C' value was 1. This means that among the tested values of 'C', 1 resulted in the highest average performance across the cross-validation folds.\n",
    "\n",
    "The cross-validation scores for the SVM model are varied (from 0.6274 to 0.9824). These scores represent the performance of the SVM model with different combinations of hyperparameters C and gamma. The best-performing SVM model has a cross-validation score of 0.9824. The corresponding hyperparameters were C = 10 and gamma = 0.01. This suggests that among the tested combinations of C and gamma, this particular combination led to the highest average performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in this step, it is possible to train the selected model with best hyperparameter values on the full training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initializing the models with the best hyperparameters\n",
    "best_logreg_model = LogisticRegression(C=best_logreg_param)\n",
    "best_svm_model = SVC(C=best_svm_param_C, gamma=best_svm_param_gamma)\n",
    "\n",
    "# Training the models on the full training data\n",
    "best_logreg_model.fit(X, y)\n",
    "best_svm_model.fit(X, y)\n",
    "\n",
    "# Making predictions on the test set: X_test & y_test are calculated in some previouse blocks\n",
    "logreg_predictions = best_logreg_model.predict(X_test)\n",
    "svm_predictions = best_svm_model.predict(X_test)\n",
    "\n",
    "#Evaluating the model performance\n",
    "logreg_accuracy = accuracy_score(y_test, logreg_predictions)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "\n",
    "print(\"Logistic Regression accuracy on test set:\", logreg_accuracy)\n",
    "print(\"SVM accuracy on test set:\", svm_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this evaluation logistic Regression model achieved an accuracy of approximate 99.12% on the test set, while the SVM model achieved an accuracy of around 98.25%. This indicates that both models are performing well on the given breast cancer dataset. But in sensitive research such as medical contexts like breast cancer prediction accuracy alone might not be the only metric to consider. It is better to examine other metrics, as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation \n",
    "\n",
    "in the following some other evaluation based on classification_report, confusion_matrix, AUC ROC, and precision-recall curve are examined.\n",
    "\n",
    "Before using functions, since dataset's dependent/target variable encoded as B and M, it is needed to convert them to binary values to prevent of errors for precision_recall_curve which need true values in binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical labels to binary values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_test_binary = label_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation for Logistic Regression\n",
    "\n",
    "logreg_probabilities = best_logreg_model.predict_proba(X_test)[:, 1]\n",
    "logreg_roc_auc = roc_auc_score(y_test, logreg_probabilities)\n",
    "logreg_precision, logreg_recall, _ = precision_recall_curve(y_test_binary, logreg_probabilities)\n",
    "logreg_pr_auc = auc(logreg_recall, logreg_precision)\n",
    "\n",
    "print(\"Logistic Regression ROC AUC:\", logreg_roc_auc)\n",
    "print(\"Logistic Regression PR AUC:\", logreg_pr_auc)\n",
    "\n",
    "logreg_predictions = best_logreg_model.predict(X_test)\n",
    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, logreg_predictions))\n",
    "logreg_confusion_matrix = confusion_matrix(y_test, logreg_predictions)\n",
    "print(\"Logistic Regression Confusion Matrix:\\n\", confusion_matrix(y_test, logreg_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation results for Logistic Regression models are performing well on the dataset.\n",
    "\n",
    "ROC AUC: 0.998 as it is close to 1, so it shows excellent discrimination capability.\n",
    "\n",
    "PR AUC: 0.997 it also close to 1 and indicats strong precision-recall trade-off.\n",
    "\n",
    "Classification Report: The model shows high precision, recall, and F1-score for both classes.\n",
    "\n",
    "Confusion Matrix: 0 false positive (Type I error) and 1 false negative (Type II error), which is good result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation for SVM model\n",
    "\n",
    "svm_probabilities = best_svm_model.decision_function(X_test)\n",
    "svm_roc_auc = roc_auc_score(y_test, svm_probabilities)\n",
    "svm_precision, svm_recall, _ = precision_recall_curve(y_test_binary, svm_probabilities)\n",
    "svm_pr_auc = auc(svm_recall, svm_precision)\n",
    "\n",
    "print(\"SVM ROC AUC:\", svm_roc_auc)\n",
    "print(\"SVM PR AUC:\", svm_pr_auc)\n",
    "\n",
    "svm_predictions = best_svm_model.predict(X_test)\n",
    "print(\"SVM Classification Report:\\n\", classification_report(y_test, svm_predictions))\n",
    "svm_confusion_matrix = confusion_matrix(y_test, svm_predictions)\n",
    "print(\"SVM Confusion Matrix:\\n\", svm_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Logistic Regression, SVM models are performing well on the dataset.\n",
    "\n",
    "ROC AUC: 0.999 as it is close to 1 it is very good discrimination.\n",
    "\n",
    "PR AUC: 0.999 it shows high precision-recall balance.\n",
    "\n",
    "Classification Report: The model has high precision, recall, and F1-score, though slightly lower for the malignant class compared to Logistic Regression.\n",
    "\n",
    "Confusion Matrix: 0 false positives (Type I error) and 2 false negatives (Type II error), seems almost good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize results\n",
    "\n",
    "In the following the above results are visualized to show better the interpretions and comparation of methods in SVM and logistic regrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting the result of Confusion Matrix\n",
    "\n",
    "class_names = ['Benign', 'Malignant']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Create heatmap for SVM\n",
    "sns.heatmap(svm_confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=class_names, yticklabels=class_names, ax=axes[0])\n",
    "axes[0].set_title(\"Confusion Matrix for SVM Results\")\n",
    "axes[0].set_xlabel(\"Predicted\")\n",
    "axes[0].set_ylabel(\"Actual\")\n",
    "\n",
    "# Create heatmap for log_reg\n",
    "sns.heatmap(logreg_confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=class_names, yticklabels=class_names, ax=axes[1])\n",
    "axes[1].set_title(\"Confusion Matrix for Logistic Regression Results\")\n",
    "axes[1].set_xlabel(\"Predicted\")\n",
    "axes[1].set_ylabel(\"Actual\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting the result of Precision & Recall \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(logreg_recall, logreg_precision, label=\"Logistic Regression (PR AUC = {:.2f})\".format(logreg_pr_auc))\n",
    "plt.plot(svm_recall, svm_precision, label=\"SVM (PR AUC = {:.2f})\".format(svm_pr_auc))\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plots the results are shown again. How ever the Classification Report is not normal to plot but other methods show the same results that were interpreted in the above section. The Confusion Matrix figures show the results in the actual and predicted status (FP, FN, TP, TN) and Precision & Recall figure shows that the models work well because values almost close to 1 and the Precision-Recall Area Under the Curve (PR AUC) is equal to 1 that shows the models are making perfect predictions, both in terms of minimizing false positives (high precision) and capturing all true positive cases (high recall)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
