{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain why it is not useful to include the column 'Roommate' in a classification procedure.\n",
    "\n",
    "Adding the Roommate column to the classification process isn't helpful, because this column just involves the label such as sequence value for each student and doesn't show any useful information for the main task (predicting test results based on roommate symptoms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Categorical Naive Bayes classifier\n",
    "\n",
    "#### making data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the dataset\n",
    "data = {\n",
    "    'shivers': ['Y', 'N', 'Y', 'N', 'N', 'Y', 'Y'],\n",
    "    'running nose': ['N', 'N', 'Y', 'Y', 'N', 'N', 'Y'],\n",
    "    'headache': ['No', 'Mild', 'No', 'No', 'Heavy', 'No', 'Mild'],\n",
    "    'test result': ['Negative', 'Negative', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive']\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dataset\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data\n",
    "\n",
    "To train a categorical naive bayes classifier on the above dataset, we need to transform the nominal data into a format that scikit-learn can work with. so in the following code get_dummies function from pandas is used to convert the categorical variables into binary (0 or 1) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "# Converting categorical variables to binary features\n",
    "df_encoded = pd.get_dummies(df.drop('test result', axis=1))\n",
    "\n",
    "# Creating the target variable\n",
    "target = df['test result']\n",
    "\n",
    "# Training the Categorical Naive Bayes classifier\n",
    "nb_classifier = CategoricalNB()\n",
    "nb_classifier.fit(df_encoded, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting\n",
    "\n",
    "Now it is possible to predict the test results for the given dataset. To show the discrepancy in the prediction for observation number 5, it is possible to manually calculate the prediction probabilities for both the Negative and Positive classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually calculating the prediction probabilities for observation 5\n",
    "observation_5 = df_encoded.iloc[4]  # 0-based index, corresponds to 5 in 1-based index\n",
    "prediction_probabilities = nb_classifier.predict_proba([observation_5])\n",
    "\n",
    "# Displaying the prediction probabilities\n",
    "print(f'Prediction probabilities: {prediction_probabilities}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction probabilities indicate that the classifier assigns a higher probability to the Negative class (0.598) than to the Positive class (0.402) for observation number 5. This higher probability for the Negative class is why the classifier predicts a Negative result for this observation, even though the actual value is Positive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize the results\n",
    "\n",
    "To visualize the results a confusion matrix and a classification report are used. The confusion matrix provides an overview of the predicted and actual classes, while the classification report gives more detailed performance metrics such as precision, recall, and F1-score for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Make predictions on the entire dataset\n",
    "predictions = nb_classifier.predict(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a confusion matrix\n",
    "confusion_mat = confusion_matrix(target, predictions)\n",
    "\n",
    "sns.heatmap(confusion_mat, annot=True, cmap='Blues')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top-left value (4) represents the number of instances that were correctly predicted as Negative (True Negative).\n",
    "\n",
    "The top-right value (0) represents the number of instances that were predicted as Positive but are actually Negative (False Positive).\n",
    "\n",
    "The bottom-left value (1) represents the number of instances that were predicted as Negative but are actually Positive (False Negative).\n",
    "\n",
    "The bottom-right value (2) represents the number of instances that were correctly predicted as Positive (True Positive).\n",
    "\n",
    "In conclusion, there are a total of 4 + 2 = 6 instances correctly predicted, and 1 instance misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a classification report\n",
    "report = classification_report(target, predictions)\n",
    "\n",
    "print('Classification Report:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report provides a comprehensive assessment of model's performance for both the negative and positive classes. The precision, recall, and F1-score values offer insights into the model's ability to correctly classify instances, and the support values give context to the class distribution. The accuracy and average F1-scores give an overall view of the model's performance.\n",
    "\n",
    "Precision: For the 'Negative' class, the precision is 0.80, indicating that 80% of the instances predicted as 'Negative' were actually 'Negative'. For the 'Positive' class, the precision is 1.00, meaning that all instances predicted as 'Positive' were actually 'Positive'.\n",
    "\n",
    "Recall: For the 'Negative' class, the recall is 1.00, indicating that all actual 'Negative' instances were correctly identified. For the 'Positive' class, the recall is 0.67, meaning that 67% of actual 'Positive' instances were correctly identified.\n",
    "\n",
    "F1-score: It provides a balanced view of the model's performance. For the 'Negative' class, the F1-score is 0.89, and for the 'Positive' class, the F1-score is 0.80.\n",
    "\n",
    "Support: Support represents the number of actual occurrences of each class in the test set. For the 'Negative' class, the support is 4, and for the 'Positive' class, the support is 3.\n",
    "\n",
    "Accuracy: The accuracy is 0.86, meaning that the model predicted the correct test result for 86% of the instances.\n",
    "\n",
    "Macro Average F1-score: The macro average F1-score is the average of the F1-scores of all classes. It gives equal weight to each class, regardless of its size. In this case, the macro average F1-score is 0.84.\n",
    "\n",
    "Weighted Average F1-score: The weighted average F1-score is the average of the F1-scores, weighted by the support of each class. It provides a measure of overall performance, considering class imbalances. In this case, the weighted average F1-score is 0.85"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
