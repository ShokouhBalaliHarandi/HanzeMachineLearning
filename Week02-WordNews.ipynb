{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Description:\n",
    "\n",
    "The main goal of the assignment is to perform topic modeling on a dataset of news articles and cluster them into different topics. \n",
    "\n",
    "This kind of analysis can be helpful for various purposes, such as understanding public senses, determining significant events, and organizing news articles navigation based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import yaml "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load & prepare data\n",
    "\n",
    "For the beginign, I loaded data and did initial investigate on them like get their shape and the type of the columns (topics). And also see the sample of it. This will help to see if the data proper for rest of the investigation and match on the format that is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "\n",
    "configPath = 'config.yaml'\n",
    "\n",
    "# Read the yaml data from the file\n",
    "with open(configPath, 'r') as file:\n",
    "    configData = yaml.safe_load(file)\n",
    "\n",
    "data = pd.read_csv(configData[\"World_News_path\"])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above results, the first column will not included in the rest of the assignmet, because it shows the date of topics and it is not important in the aim of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing data\n",
    "\n",
    "The text data from the columns \"Top1\" to \"Top25\" is extracted and then cleaned using the clean_text function. The text cleaning involves removing unwanted characters and converting all text to lowercase. By this function texts are made more similar in format and model can better recognize the different context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from columns\n",
    "text_data = data.loc[:, \"Top1\":\"Top25\"].values.flatten().tolist()\n",
    "\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # Remove unwanted characters\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # Remove more than 2 space\n",
    "    text = text.strip() #remove any space in the first or last part of text\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = [clean_text(text) if isinstance(text, str) else \"\" for text in text_data]\n",
    "\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown all characters except characters and 1 space between words are removed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert data to Document-Term Matrix (DTM)\n",
    "\n",
    "In this step, cleaned text was converted into a Document-Term Matrix (DTM) using the TF-IDF vectorization technique. This matrix represents the frequency of words in each text entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words=\"english\")\n",
    "\n",
    "# Fit and transform the text data\n",
    "dtm = vectorizer.fit_transform(cleaned_text)\n",
    "\n",
    "# Convert the DTM to a dense array\n",
    "dtm_array = dtm.toarray()\n",
    "\n",
    "dtm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### applying Non-Negative matrix factorization (NMF)\n",
    "\n",
    "The Non-Negative Matrix Factorization (NMF) algorithm is applied to the DTM. NMF is an unsupervised machine learning algorithm commonly used for text clustering. NMF is modeling technique that decomposes the DTM into two matrices, one representing the document-topic distribution and the other representing the topic-term distribution. \n",
    "\n",
    "For NMF we have to set cluster amount to recognize the proper number of cluster, the elbow method is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculating sum of square of different numbers of clusters\n",
    "wcss = []\n",
    "max_cluster = 10 #max clustering based on 10\n",
    "for num_cluster in range(1, max_cluster + 1):\n",
    "    nmf = NMF(n_components = max_cluster, random_state = 42)\n",
    "    nmf.fit(dtm.toarray())\n",
    "    wcss.append(nmf.reconstruction_err_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Elbow Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, max_cluster + 1), wcss, marker='o')\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Within-cluster Sum of Squares (WCSS)\")\n",
    "plt.title(\"Elbow method to find proper number of clusters\")\n",
    "plt.xticks(range(1, max_cluster + 1))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the above plot drived from elbow method, the result is a line with a relatively fixed slope. (the WCSS amount is between 201 and 207). As I didn't want to have more than 10 cluster, so this plot can show us: the data may not have distinct clusters in our range.\n",
    "\n",
    "It seems it may be difficult to determine the well-defined number of clusters with the elbow Method alone. So, the silhouette is used to double check the quality of clustering in max_clustering 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "max_cluster = 10\n",
    "silhouette_scores = []\n",
    "for num_cluster in range(2, max_cluster + 1):\n",
    "    nmf = NMF(n_components = num_cluster, random_state = 42)\n",
    "    nmf.fit(dtm.toarray())\n",
    "    cluster_labels = nmf.transform(dtm.toarray()).argmax(axis=1)\n",
    "    silhouette_scores.append(silhouette_score(dtm.toarray(), cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Finding the proper number of clusters based on highest Silhouette Score\n",
    "optimal_num_clusters = np.argmax(silhouette_scores) + 2 # the 2 added because the loop is started from 2\n",
    "\n",
    "print(\"Proper number of clusters:\", optimal_num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the silhouette scores I picked the 3 clusters for the rest. (However, maybe other methods is exists for this aim and suggest other numbers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the NMF, the clusters and their corresponding topics are obtained. For accessing the cluster labels nmf.transform(dtm_array) is used. Each document will have a corresponding vector indicating its membership probabilities for each cluster. (It is notable that each document is assigned to one of the 3 clusters based on the highest probability in the document-topic matrix.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3\n",
    "\n",
    "nmf = NMF(n_components=num_clusters, random_state=42)\n",
    "nmf.fit(dtm_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performing results\n",
    "\n",
    "The result are investigated in these steps:\n",
    "\n",
    "1. Getting the 10 top words from each cluster: the result is shown based on the importance given by NMF. \n",
    "\n",
    "2. Assigning documents to clusters: Assigning each document to a cluster based on the highest probability in the document-topic matrix.\n",
    "\n",
    "3. Visualization: With t-SNE document-topic matrix is visualed in to 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Getting the 10 top words from each cluster\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "num_top_words = 10  # Define the number of top words to display\n",
    "\n",
    "# Get the top words for each topic\n",
    "for i, component in enumerate(nmf.components_):\n",
    "    top_words_indices = component.argsort()[:-num_top_words-1:-1]\n",
    "    top_words = [feature_names[idx] for idx in top_words_indices]\n",
    "    print(f\"Cluster/Topic {i+1}: {', '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result is shown 10 top word in each clusters.\n",
    "\n",
    "In the following assign each documnet (expression/topic in the main dataset) to one the clusters based on the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. get the document-topic matrix\n",
    "doc_topic_matrix = nmf.transform(dtm_array)\n",
    "\n",
    "# Assign documents to clusters based on the highest probability\n",
    "cluster_labels = np.argmax(doc_topic_matrix, axis=1)\n",
    "\n",
    "# Print the assigned cluster for each document\n",
    "for i, cluster_label in enumerate(cluster_labels):\n",
    "    print(f\"Document {i+1} belongs to Cluster {cluster_label+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the shape of topics were 1859 rows and 25 topics (with out date column as illustrated before) so we have 46475 (=25*1859) document. in the above all 46475 document are assigned to one cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization with t-sne in 2D.\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Apply t-SNE to reduce dimensionality to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "doc_topic_tsne = tsne.fit_transform(doc_topic_matrix)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.scatter(doc_topic_tsne[:, 0], doc_topic_tsne[:, 1], c=cluster_labels)\n",
    "plt.title('Clustering of Documents')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2D scatter plot displays the results of text clustering using NMF on the news headlines dataset. Each point represents a news headline, and the color of the points corresponds to their assigned cluster. Clusters appear to be closer together, suggesting higher similarity among documents in them. The clusters are not completely seprated from each other and there are some kind of noise among of them. From the above plot can be concluded that the clustering could be improved.\n",
    "\n",
    "Also, the distrbution of documents between clusters have high difference. To be sure about this the following code is added to get the amount of documents in each cluster. (for this the cluster_labels that calculated in the \"Assigning documents to clusters\" is used.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each cluster label\n",
    "cluster_counts = np.bincount(cluster_labels)\n",
    "\n",
    "# Print the number of documents in each cluster\n",
    "for cluster_num, doc_count in enumerate(cluster_counts):\n",
    "    print(f\"Cluster {cluster_num + 1}: {doc_count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of documents that used in each cluster are printted in the above and it is similar with the t-sne result. (big differences in distribution. For instance the smallest cluster has 393 however bigger one has 40837 documents.)\n",
    "\n",
    "For conclusion, data needs more analysing and investigating. Clustering news topics can be compare by other ways because by the result of this assignment there is some kind of similarity between headlines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
